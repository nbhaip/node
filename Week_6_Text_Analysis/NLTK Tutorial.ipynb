{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "may need to update nltk: pip install -U nltk\n",
    "\n",
    "http://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Things\n",
    "\n",
    "after running the below line of code, a window should pop up. Press \"all\" then press \"download\". Boom. Everything you could ever want for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a1a554e5d735>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;31m# function should make a new copy of self to use?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdownload_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interactive_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mTKINTER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m                 \u001b[0mDownloaderGUI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTclError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m                 \u001b[0mDownloaderShell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1709\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m     \u001b[0;31m#/////////////////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/tkinter/__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terms to know:\n",
    "\n",
    "Tokenizing - word tokenizers, sentence tokenizers\n",
    "\n",
    "Stop Words - meaningless words (or filler words) such as \"like\", \"as\", \"the\", \"it\", etc…\n",
    "\n",
    "Corpora (Corpus) - body of text. ex: medical journals, presidential speeches, etc…\n",
    "\n",
    "Lexicon - words and their meanings. ex: \"well\" can beither an adjective or a noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized by Sentence:\n",
      "\n",
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great and python is awesome.', 'The sky is pinkish-blue.', 'You should not eat cardboard.']\n",
      "\n",
      "Tokenized by word:\n",
      "\n",
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', 'not', 'eat', 'cardboard', '.']\n",
      "\n",
      "Tokenized by word without punctuation:\n",
      "\n",
      "['Hello', 'Mr', 'Smith', 'how', 'are', 'you', 'doing', 'today', 'The', 'weather', 'is', 'great', 'and', 'python', 'is', 'awesome', 'The', 'sky', 'is', 'pinkish', 'blue', 'You', 'should', 'not', 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "example_text = 'Hello Mr. Smith, how are you doing today? The weather is great and python is awesome. The sky is pinkish-blue. You should not eat cardboard.'\n",
    "\n",
    "# tokenize by sentence\n",
    "print('Tokenized by Sentence:\\n')\n",
    "print(sent_tokenize(example_text))\n",
    "\n",
    "\n",
    "# tokenize by word\n",
    "print('\\nTokenized by word:\\n')\n",
    "print(word_tokenize(example_text))\n",
    "# notice that this counts punctuation as its own word. You can change this parameter if you so choose.\n",
    "\n",
    "# if you want to tokenize the words but exclude punctuation, use this RegexpTokenizer():\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "print('\\nTokenized by word without punctuation:\\n')\n",
    "print(tokenizer.tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words:\n",
      "\n",
      "{'mightn', 'other', 'haven', 'do', 'mustn', 've', 'had', 'needn', 'that', 'off', 'from', 'you', 'above', 'for', 'into', 'too', 'over', 'couldn', 'itself', 'then', 'these', 'been', 'before', 'after', 'have', 'they', 'ain', 'under', 'myself', 'than', 'who', 'below', 'our', 'is', 'isn', 'ours', 'more', 'should', 'them', 'the', 'any', 'didn', 'my', 'her', 'nor', 's', 'as', 'being', 'very', 'those', 'me', 'themselves', 'shan', 'hasn', 'ma', 'so', 'she', 'because', 'has', 'did', 'why', 'further', 'of', 'no', 'hers', 'doing', 'out', 'won', 'yours', 'most', 'theirs', 'by', 'against', 'and', 'up', 'during', 'where', 're', 'only', 'or', 'your', 'a', 'aren', 'was', 'once', 'can', 'it', 'when', 'be', 'does', 'yourselves', 'himself', 'each', 'him', 'through', 'while', 'am', 'to', 'what', 'with', 'whom', 'doesn', 'are', 'but', 'herself', 't', 'yourself', 'there', 'wouldn', 'down', 'its', 'i', 'same', 'an', 'some', 'shouldn', 'he', 'how', 'such', 'until', 'now', 'between', 'at', 'were', 'his', 'having', 'on', 'weren', 'we', 'will', 'm', 'hadn', 'this', 'in', 'y', 'll', 'if', 'don', 'wasn', 'about', 'both', 'their', 'again', 'ourselves', 'few', 'just', 'here', 'all', 'own', 'which', 'o', 'd', 'not'}\n",
      "\n",
      "Examples of removing stop words:\n",
      "\n",
      "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n",
      "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "example_sentence = 'This is an example showing off stop word filtration.'\n",
    "\n",
    "# NLTK has multiple sets of stop words, but we are going to use the set made specifically for the English language.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print('Stop words:\\n')\n",
    "print(stop_words)\n",
    "\n",
    "print('\\nExamples of removing stop words:\\n')\n",
    "      \n",
    "words = word_tokenize(example_sentence)\n",
    "\n",
    "# non-fancy way to filter the stop words out of your text:\n",
    "filtered_sentence = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(filtered_sentence)\n",
    "# notice that only the \"important\" words are left\n",
    "\n",
    "# fancy way to filter the stop words out of your text:\n",
    "filtered_sentence = [word for word in words if not word in stop_words] # basically the for-loop in a single line\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stemming\n",
    "\n",
    "Taking words and \"stemming\" the ends of them. Such as \"-ed\", or \"-ing\" at the end of a word.\n",
    "\n",
    "So, the words \"worked\" and \"working\" would both be converted to \"work\" because they basically have the same root meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example words:\n",
      "\n",
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n",
      "\n",
      "Stemmed sentence:\n",
      "\n",
      "It\n",
      "is\n",
      "veri\n",
      "import\n",
      "to\n",
      "be\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = ['python','pythoner','pythoning','pythoned','pythonly']\n",
    "\n",
    "print('Example words:\\n')\n",
    "for word in example_words:\n",
    "    print(ps.stem(word))\n",
    "\n",
    "print('\\nStemmed sentence:\\n')\n",
    "# Lets see what a stemmed sentence looks like\n",
    "sent_to_stem = 'It is very important to be pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.'\n",
    "\n",
    "words = word_tokenize(sent_to_stem)\n",
    "# we tokenize because PorterStemmer can only stem one word at a time, not an entire sentence.\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming can sometimes act strangely with \"-ly\" endings and, as you can see above, the word \"once\", but overall serves its purpose to take different forms of words and convert them to their root meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PRESIDENT', 'NNP'),\n",
       " ('GEORGE', 'NNP'),\n",
       " ('W.', 'NNP'),\n",
       " ('BUSH', 'NNP'),\n",
       " (\"'S\", 'POS'),\n",
       " ('ADDRESS', 'NNP'),\n",
       " ('BEFORE', 'IN'),\n",
       " ('A', 'NNP'),\n",
       " ('JOINT', 'NNP'),\n",
       " ('SESSION', 'NNP'),\n",
       " ('OF', 'IN'),\n",
       " ('THE', 'NNP'),\n",
       " ('CONGRESS', 'NNP'),\n",
       " ('ON', 'NNP'),\n",
       " ('THE', 'NNP'),\n",
       " ('STATE', 'NNP'),\n",
       " ('OF', 'IN'),\n",
       " ('THE', 'NNP'),\n",
       " ('UNION', 'NNP'),\n",
       " ('January', 'NNP')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "from nltk.corpus import state_union # imports a pre-loaded corpus with a bunch of state of the union addresses\n",
    "\n",
    "# get our state of the union address to analyze\n",
    "sample_text = state_union.raw(\"2006-GWBUSH.txt\")\n",
    "\n",
    "# tokenize each word of the text\n",
    "words = nltk.word_tokenize(sample_text)\n",
    "\n",
    "# use nltk.pos_tag() on the words to get their parts of speech in one long list\n",
    "tagged = nltk.pos_tag(words)\n",
    "\n",
    "tagged[0:20] # only printed the first 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part of speech tags:\n",
    "\n",
    "CC - coordinating conjunction<br>\n",
    "CD - cardinal digit<br>\n",
    "DT - determiner<br>\n",
    "EX - existential there (like: \"there is\" ... think of it like \"there exists\")<br>\n",
    "FW - foreign word<br>\n",
    "IN - preposition/subordinating conjunction<br>\n",
    "JJ - adjective\t'big'<br>\n",
    "JJR - adjective, comparative\t'bigger'<br>\n",
    "JJS - adjective, superlative\t'biggest'<br>\n",
    "LS - list marker\t1)<br>\n",
    "MD - modal\tcould, will<br>\n",
    "NN - noun, singular 'desk'<br>\n",
    "NNS - noun plural\t'desks'<br>\n",
    "NNP - proper noun, singular\t'Harrison'<br>\n",
    "NNPS - proper noun, plural\t'Americans'<br>\n",
    "PDT - predeterminer\t'all the kids'<br>\n",
    "POS - possessive ending\tparent's<br>\n",
    "PRP - personal pronoun\tI, he, she<br>\n",
    "PRP\\$ - possessive pronoun\tmy, his, hers<br>\n",
    "RB - adverb\tvery, silently<br>\n",
    "RBR - adverb, comparative\tbetter<br>\n",
    "RBS - adverb, superlative\tbest<br>\n",
    "RP - particle\tgive up<br>\n",
    "TO - to\tgo 'to' the store.<br>\n",
    "UH - interjection\terrrrrrrrm<br>\n",
    "VB - verb, base form\ttake<br>\n",
    "VBD - verb, past tense\ttook<br>\n",
    "VBG - verb, gerund/present participle\ttaking<br>\n",
    "VBN - verb, past participle\ttaken<br>\n",
    "VBP - verb, sing. present, non-3d\ttake<br>\n",
    "VBZ - verb, 3rd person sing. present\ttakes<br>\n",
    "WDT - wh-determiner\twhich<br>\n",
    "WP - wh-pronoun\twho, what<br>\n",
    "WP$ - possessive wh-pronoun\twhose<br>\n",
    "WRB - wh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 11199), ('movi', 6977), ('one', 6029), ('like', 4137), ('charact', 3881), ('make', 3243), ('get', 3220), ('time', 3047), ('scene', 2671), ('even', 2611), ('good', 2475), ('play', 2382), ('stori', 2345), ('see', 2224), ('would', 2109), ('much', 2050), ('go', 2015), ('well', 1968), ('also', 1967), ('two', 1911)]\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from nltk.corpus import movie_reviews # imports a corpus with a bunch of movie reviews\n",
    "import re\n",
    "\n",
    "\n",
    "# so that we are not finding the frequency of more or less useless things, lets remove stop words and punctuation\n",
    "# all of the words in the corpus:\n",
    "words = movie_reviews.words()\n",
    "# removing stop words:\n",
    "stop_words = set(stopwords.words('english')) # we make this a set because sets are faster than lists\n",
    "filtered_words = [word for word in words if not word in stop_words]\n",
    "\n",
    "\n",
    "# define \"all_words\" as a list that will be all of the words in the corpus (converted to lowercase)\n",
    "all_words = []\n",
    "for word in filtered_words: # movie_reviews.words() is a list of all of the documents tokenized by word\n",
    "    # lets stem and convert the words to lowercase so that we avoid duplicated words and get more meaningful results\n",
    "    if re.match('[a-zA-Z]+', word): # filtering out punctuation\n",
    "        try:\n",
    "            # stem the word\n",
    "            word = ps.stem(word)\n",
    "        except IndexError: \n",
    "            pass\n",
    "        finally:\n",
    "            # convert word to lowercase and append it to all_words\n",
    "            all_words.append(word.lower())\n",
    "\n",
    "\n",
    "# define \"word_frequencies\" as the frequencies of all of the words in \"all_words\"\n",
    "word_frequencies = nltk.FreqDist(all_words)\n",
    "# print the 20 most common words\n",
    "print(word_frequencies.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
